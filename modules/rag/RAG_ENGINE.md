# RAG-Engine

## √úbersicht

Die RAG-Engine (Retrieval Augmented Generation) ist das Herzst√ºck des nscale DMS Assistent und verbindet lokale Sprachmodelle mit einem intelligenten Dokumentenretrieval-System. Diese Komponente erm√∂glicht es dem System, Benutzeranfragen zu kontextualisieren, indem sie relevante Dokumente und Informationen aus der Dokumentensammlung identifiziert und in die Antwortgenerierung einbezieht.

## Architektur

Die RAG-Engine besteht aus mehreren zusammenwirkenden Komponenten:

```
+---------------+     +---------------+     +---------------+
| LLM-Client    |<--->| RAG-Engine    |<--->| Fallback      |
| (Ollama)      |     | (Core)        |     | Search        |
+---------------+     +---------------+     +---------------+
                            ^  ^
                            |  |
                  +---------+  +---------+
                  |                      |
          +---------------+     +---------------+
          | Document      |     | Embedding     |
          | Store         |     | Manager       |
          +---------------+     +---------------+
```

### Hauptkomponenten

1. **RAG-Engine (Core)**
   - Zentrale Steuerungseinheit f√ºr den RAG-Prozess
   - Koordiniert die Zusammenarbeit zwischen LLM, Retrieval und Dokumentenverarbeitung
   - Implementiert Streaming-Funktionalit√§t f√ºr Echtzeit-Antworten
   - Intelligent Thread-Management f√ºr parallele Verarbeitung

2. **LLM-Client (OllamaClient)**
   - Schnittstelle zum lokalen Sprachmodell (LLama3)
   - Verarbeitet Prompts und generiert Antworten
   - Unterst√ºtzt Streaming-Ausgabe f√ºr Echtzeit-Interaktion
   - Optimiert f√ºr lokale Ausf√ºhrung ohne externe APIs

3. **Document Store**
   - Verwaltet die Dokumente und ihre Metadaten
   - L√§dt und indexiert Dokumente aus dem Dateisystem
   - Bietet Schnittstellen f√ºr die Abfrage von Dokumenteninhalten

4. **Embedding Manager**
   - Erstellt Vektorrepr√§sentationen von Texten
   - Unterst√ºtzt semantische √Ñhnlichkeitssuche
   - Implementiert effiziente √Ñhnlichkeitsberechnungen
   - Verwaltet Embedding-Cache f√ºr verbesserte Leistung

5. **Fallback Search**
   - Alternative Suchmethoden, wenn die semantische Suche keine zufriedenstellenden Ergebnisse liefert
   - Implementiert Schl√ºsselwort- und regul√§re Ausdrucksuche
   - Erm√∂glicht hybride Suchans√§tze f√ºr maximale Abdeckung

## Prozessablauf

Die RAG-Engine arbeitet in folgenden Schritten:

1. **Anfrage-Analyse**
   - Benutzeranfrage wird analysiert und in Embedding umgewandelt
   - Themenerkennung und Schl√ºsselwort-Extraktion

2. **Dokumenten-Retrieval**
   - Semantische Suche nach relevanten Dokumenten
   - Berechnung der √Ñhnlichkeitsscores zwischen Anfrage und Dokumenten
   - Auswahl der besten √úbereinstimmungen

3. **Kontext-Aufbereitung**
   - Zusammenstellung der relevanten Dokumentenabschnitte
   - Formatierung f√ºr optimale LLM-Verarbeitung
   - Priorisierung basierend auf Relevanzscores

4. **Prompt-Erstellung**
   - Erstellen eines strukturierten Prompts mit Benutzeranfrage und Dokumentenkontext
   - Anweisungen f√ºr das LLM zur Antwortgenerierung
   - Richtlinien zur Korrekten Quellenverwendung

5. **Antwortgenerierung**
   - √úbergabe des Prompts an das LLM
   - Echtzeit-Streaming der Antwort w√§hrend der Generierung
   - Quellenr√ºckverfolgung und Zitierung

## Hauptmerkmale

### 1. Asynchrone Verarbeitung

Die RAG-Engine nutzt Pythons asynchrone Funktionalit√§t f√ºr parallele Verarbeitung:

```python
async def generate_answer(self, query: str, session_id: Optional[int] = None, 
                         stream: bool = False) -> Dict[str, Any]:
    """Generiert eine Antwort basierend auf einer Anfrage mit RAG"""
    # Asynchrone Verarbeitung der Anfrage
    # ...
```

### 2. Streaming-Unterst√ºtzung

Echtzeit-Antworten werden √ºber Server-Sent Events (SSE) gestreamt:

```python
async def stream_response(self, query: str, session_id: Optional[int] = None) -> AsyncGenerator[str, None]:
    """Streamt die Antwort auf eine Anfrage in Echtzeit"""
    # Stream-ID generieren und verwalten
    # ...
    try:
        async for token in self.ollama_client.generate_stream(prompt, model_name):
            yield token
    except Exception as e:
        logger.error(f"Fehler beim Streaming: {e}")
        yield json.dumps({"error": str(e)})
```

### 3. Hardware-Optimierung

Die Engine erkennt und nutzt automatisch verf√ºgbare Hardware-Beschleunigung:

```python
# Erkenne CUDA-Unterst√ºtzung
if torch.cuda.is_available():
    self.device = "cuda"
    self.torch_dtype = torch.float16
    logger.info("üöÄ CUDA ist verf√ºgbar ‚Äì GPU wird verwendet.")
else:
    self.device = "cpu"
    self.torch_dtype = torch.float32
    logger.warning("‚ö†Ô∏è CUDA nicht verf√ºgbar ‚Äì es wird die CPU verwendet.")
```

### 4. Kontextualisierte Antworten

Antworten werden mit Quellenangaben und Dokumentenreferenzen angereichert:

```python
# Beispiel f√ºr eine Antwort mit Quellenangaben
response = {
    "answer": "Die nscale Dokumentenverwaltung unterst√ºtzt...",
    "sources": [
        {"document": "nscale_handbuch.pdf", "page": 42, "relevance": 0.92},
        {"document": "technische_spezifikation.docx", "section": "3.1", "relevance": 0.85}
    ],
    "processing_time": 1.23
}
```

### 5. Fallback-Strategien

Bei unzureichenden Ergebnissen werden alternative Suchmethoden aktiviert:

```python
# Wenn semantische Suche nicht gen√ºgend Ergebnisse liefert
if len(results) < min_results:
    logger.info(f"Zu wenige Ergebnisse ({len(results)}), verwende Fallback-Suche")
    fallback_results = self.fallback_search.keyword_search(query, top_k=min_results)
    results.extend(fallback_results)
```

## Prompt-Gestaltung

Die RAG-Engine verwendet sorgf√§ltig gestaltete Prompts f√ºr optimale Antworten:

```
# Beispiel f√ºr einen RAG-Prompt

Sie sind ein Assistent f√ºr das nscale DMS-System. 
Beantworten Sie die folgende Frage basierend auf den bereitgestellten Dokumenten.
Wenn Sie die Antwort nicht in den Dokumenten finden, sagen Sie ehrlich, dass Sie es nicht wissen.

FRAGE: {query}

KONTEXT:
{context}

Verwenden Sie die Quellen in Ihrer Antwort und geben Sie die entsprechenden Referenzen an.
```

## Konfiguration

Die Engine ist √ºber verschiedene Parameter konfigurierbar:

| Parameter | Beschreibung | Standardwert |
|-----------|--------------|--------------|
| `max_context_length` | Maximale L√§nge des Kontexts f√ºr das LLM | 4000 |
| `similarity_threshold` | Mindest-√Ñhnlichkeitsschwelle f√ºr Dokumente | 0.5 |
| `num_retrieval_results` | Anzahl der auszuw√§hlenden Dokumente | 5 |
| `device` | Zu verwendende Hardware (cuda/cpu) | Automatisch |
| `use_hybrid_search` | Kombiniert semantische und Keyword-Suche | True |

## Optimierung der Antwortqualit√§t

Die RAG-Engine implementiert mehrere Strategien zur Qualit√§tsverbesserung:

1. **Query Expansion**:
   - Anreicherung der Suchanfrage mit synonymen Begriffen
   - Erweiterung um fachspezifische Terminologie

2. **Re-Ranking**:
   - Zweistufige Bewertung der gefundenen Dokumente
   - Ber√ºcksichtigung von Aktualit√§t und Relevanz

3. **Dynamische Kontextanpassung**:
   - Intelligente Auswahl der wichtigsten Dokumentabschnitte
   - Optimale Nutzung des begrenzten Kontextfensters des LLM

## Leistungsmetriken

Die Engine erfasst verschiedene Metriken zur Leistungs√ºberwachung:

- **Antwortzeit**: Zeit von der Anfrage bis zur vollst√§ndigen Antwort
- **Retrieval-Qualit√§t**: Relevanz der gefundenen Dokumente
- **Speichernutzung**: √úberwachung des RAM- und VRAM-Verbrauchs
- **Cache-Trefferrate**: Effizienz des Embedding-Caches

## Integration

Die RAG-Engine wird √ºber die Server-API in die Gesamtanwendung integriert:

```python
@app.post("/api/chat")
async def chat(request: ChatRequest, current_user: dict = Depends(get_current_user)):
    """Chat-Endpunkt f√ºr die Kommunikation mit dem LLM"""
    if request.stream:
        # Streaming-Antwort zur√ºckgeben
        return StreamingResponse(
            rag_engine.stream_response(request.message, request.session_id),
            media_type="text/event-stream"
        )
    else:
        # Normale Antwort zur√ºckgeben
        response = await rag_engine.generate_answer(
            request.message, 
            request.session_id,
            stream=False
        )
        return response
```

## Erweiterbarkeit

Die RAG-Engine ist f√ºr zuk√ºnftige Erweiterungen konzipiert:

1. **Modell-Austauschbarkeit**: Einfacher Wechsel zwischen verschiedenen LLMs
2. **Neue Retrieval-Methoden**: Integration zus√§tzlicher Suchalgorithmen
3. **Multimodale Unterst√ºtzung**: M√∂glichkeit zur Einbindung von Bild- und Audioanalyse
4. **Personalisierung**: Anpassung der Antworten an Benutzerhistorie und -pr√§ferenzen

---

Aktualisiert: 04.05.2025